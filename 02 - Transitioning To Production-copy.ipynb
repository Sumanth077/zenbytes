{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZenML: Open-source MLOps Framework for reproducible ML pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](_assets/Logo/zenml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> This lesson is still in progress and some commands may not work as described. Please expect an update until 20th April 2022. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging as absl_logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "absl_logging.set_verbosity(-10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by initializing ZenML in our directory. We are going to use a local stack to begin with, for simplicity and then transition to other stacks. This can be achieved in code by executing the following block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize ZenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf .zen\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML handles integrations natively, to avoid dependency conflicts, so make sure to use the following command to install the integrations required for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![All](_assets/integrations_all.png \"All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install kubeflow seldon s3 -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Concept of MLOps Stacks\n",
    "\n",
    "The ZenML stack is a concept that describes the union of Metadata Store, Artifact Store and Orchestrator that will be used for all pipeline runs. When you get started with zenml you start off with a default local stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Local Stack\n",
    "\n",
    "You can imagine the local stack to look like this. Within the diagram we show how a generic pipeline interacts with the local stack.\n",
    "\n",
    "![LocalStack](_assets/localstack.png \"LocalStack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kubeflow Pipelines stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the Kubeflow integration to extend the concept of stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to transition to a kubeflow stack that will look a little bit like this. Note that for kubeflow pipelines we also need a registry where the docker images for each step are registered. \n",
    "\n",
    "![KubeflowStack](_assets/aws_stack_redesigned.png \"KubeflowStack\")\n",
    "\n",
    "But we have good news! You barely have to do anything to transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitioning to Production with Kubeflow on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps to follow in order to continue.\n",
    "\n",
    "- Set up the neccessary cloud resources on the provider of your choice\n",
    "- Configure ZenML with a new stack to be able to communicate with these resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up using the cloud guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to continue, it is best to follow the updated cloud guide for ZenML found [here](https://docs.zenml.io/features/guide-aws-gcp-azure). Please return after finishing the `pre-requisites` section.\n",
    "\n",
    "It is recommended you use AWS as your cloud provider to follow along the lesson. However, if you were to select GCP or Azure, it should not so hard to actually modify the below commands to work accordingly.\n",
    "\n",
    "You will also need Seldon Core to be installed in the same Kubernetes cluster as Kubeflow. Some brief instructions on how to install Seldon Core in AWS EKS can be found [here](https://github.com/zenml-io/zenml/tree/main/examples/seldon_deployment#installing-seldon-core-eg-in-an-eks-cluster). It is also advisable to read the official [Seldon Core documentation](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your AWS Kubeflow Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can configure a new stack that points to your newly created resources on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember from the main README, Kubernetes and Docker are a pre-requisite to this part of the guide. Please make sure you have them installed. You also need to install the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) to move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the following with your own configuration. Use the below as exemplary.\n",
    "\n",
    "AWS_EKS_CLUSTER=\"zenhacks-cluster\"\n",
    "AWS_REGION=\"us-east-1\"\n",
    "ECR_REGISTRY_NAME=\"715803424590.dkr.ecr.us-east-1.amazonaws.com\"\n",
    "S3_BUCKET_NAME=\"s3://zenbytes-bucket\"\n",
    "KUBEFLOW_NAMESPACE=\"kubeflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up local access to the AWS EKS cluster and the AWS ECR registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Docker to the ECR registry\n",
    "!aws ecr get-login-password --region {AWS_REGION} | docker login --username AWS --password-stdin {ECR_REGISTRY_NAME}\n",
    "\n",
    "# Create a Kubernetes configuration context that points to the EKS cluster\n",
    "!aws eks --region {AWS_REGION} update-kubeconfig --name {AWS_EKS_CLUSTER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set up a Kubernetes Secret to give Seldon Core access to the AWS S3 artifact store in the configured namespace.\n",
    "\n",
    "NOTE: this is based on the assumption that Seldon Core is running in an EKS cluster that already has IAM access enabled and doesn't need any explicit AWS credentials. If that is not the case, you will need to set up credentials differently. Please look up the variables relevant to your use-case in the official [Seldon Core documentation](https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html#handling-credentials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n {KUBEFLOW_NAMESPACE} create secret generic seldon-init-container-secret --from-literal=RCLONE_CONFIG_S3_PROVIDER='aws' --from-literal=RCLONE_CONFIG_S3_TYPE='s3' --from-literal=RCLONE_CONFIG_S3_ENV_AUTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the base URL that will be used by Seldon Core to expose all model servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INGRESS_HOST = ! echo $(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n",
    "INGRESS_HOST[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register the ZenML Stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register container registry\n",
    "!zenml container-registry register ecr_registry --type=default --uri={ECR_REGISTRY_NAME}\n",
    "\n",
    "# Register orchestrator (Kubeflow on AWS)\n",
    "!zenml orchestrator register eks_orchestrator --type=kubeflow \n",
    "\n",
    "# Register metadata store and artifact store\n",
    "!zenml metadata-store register kubeflow_metadata_store --type=kubeflow\n",
    "!zenml artifact-store register s3_store --type=s3 --path={S3_BUCKET_NAME}\n",
    "\n",
    "# Register the Seldon Core model deployer (Seldon on AWS)\n",
    "!zenml model-deployer register eks_seldon --type=seldon --kubernetes_namespace={KUBEFLOW_NAMESPACE} --base_url=http://{INGRESS_HOST[0]}\n",
    "\n",
    "# Register a secret manager\n",
    "!zenml secrets-manager register local_secret_manager --type=local\n",
    "\n",
    "# Register the aws_kubeflow_stack\n",
    "!zenml stack register aws_kubeflow_stack -m kubeflow_metadata_store -a s3_store -o eks_orchestrator -c ecr_registry -d eks_seldon -x local_secret_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition to Production (Run on the Cloud)\n",
    "\n",
    "Once the stack is configured, all that is left to do is to set it active and to run a pipeline. Note that the code itself DOES NOT need to change, only the active stack.\n",
    "\n",
    "ZenML will detect that the stack has changed, and instead of running your pipeline locally, will build a Docker Image, push it to the container registry with your requirements, and deploy the pipeline with that image on Kubeflow Pipelines. This whole process is usually very painful but simplified with ZenML, and is completely customizable.\n",
    "\n",
    "For now, try it out! It might take a few minutes to build and push the image, but after that you'd see your pipeline in the cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Currently running pipelines defined within a jupyter notebook cell is\n",
    "    not supported. To get around this you can run the train pipeline within this repo. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zenml stack set aws_kubeflow_stack\n",
    "!zenml integration uninstall -f mlflow\n",
    "\n",
    "# Let's train within kubeflow pipelines - this will deploy the pipeline\n",
    "!python run.py --deploy --predict # --interval-second=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the pipeline run, you should port-forward Kubeflow Pipelines to: [http://localhost:8080/](http://localhost:8080/). You might want to try this is a seperate shell:\n",
    "\n",
    "```\n",
    "kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this only if the port forward from `zenml stack up` did not work. \n",
    "!kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it you have successfully transitioned from local to production by simply switching you ZenML stack. This is just scratching the surface!\n",
    "\n",
    "Next up, more about stacks, running pipelines on a schedule, and much more coming soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
