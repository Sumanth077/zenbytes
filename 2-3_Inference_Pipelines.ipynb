{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2-3: Inference Pipelines\n",
    "\n",
    "***Key Concepts:*** *Inference Pipelines*\n",
    "\n",
    "In the last lesson we have learned how to add model deployment as a step in our ML pipeline, which allows us to automatically deploy models into production after training them. We also saw how to manually interact with the served model.\n",
    "\n",
    "In practice, querying the model is just one of many steps you would have to perform at inference time. Whenever you receive a request, you might need to preprocess the data you received, and you might also have some postprocessing code that you want to run after your model, like converting outputs to a different format, sending alerts, etc.\n",
    "\n",
    "That is why it makes sense to not only use ML pipelines for model training, but for inference as well.\n",
    "\n",
    "![Training and Inference Pipelines GIF](_assets/2-3/training_inference_pipelines.gif)\n",
    "\n",
    "In this notebook we will build a very basic inference pipeline to interact with our served model. \n",
    "The pipeline will consist of the following three steps:\n",
    "1. Load a data sample\n",
    "2. Load the model (prediction service)\n",
    "3. Inference the model on the data sample\n",
    "\n",
    "Let's define such a pipeline in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def inference_pipeline(\n",
    "    inference_data_loader,\n",
    "    prediction_service_loader,\n",
    "    predictor,\n",
    "):\n",
    "    \"\"\"Basic inference pipeline.\"\"\"\n",
    "    inference_data = inference_data_loader()\n",
    "    model_deployment_service = prediction_service_loader()\n",
    "    predictor(model_deployment_service, inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the inference data loader might receive a single sample from an API request, or it might load a batch of data from a data lake or similar. For simplicity, we will mock this component for now and just load an 8x8 random noise image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from zenml.steps import step\n",
    "\n",
    "\n",
    "@step\n",
    "def inference_data_loader() -> np.ndarray:\n",
    "    \"\"\"Load some inference data.\"\"\"\n",
    "    return np.random.rand(1, 64)  # flattened 8x8 random noise image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the `prediction_service_loader` step. We can use the exact same code here that we used for manually querying the model service in the last lesson, just wrapped in a ZenML step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step, Output\n",
    "from zenml.services import BaseService\n",
    "from zenml.repository import Repository\n",
    "\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> BaseService:\n",
    "    \"\"\"Load the model service of our train_evaluate_deploy_pipeline.\"\"\"\n",
    "    repo = Repository(skip_repository_check=True)\n",
    "    model_deployer = repo.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"train_evaluate_deploy_pipeline\",\n",
    "        pipeline_step_name=\"mlflow_model_deployer_step\",\n",
    "        running=True,\n",
    "    )\n",
    "    service = services[0]\n",
    "    return service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's write the `predictor` step that will inference our served model on the inference data sample. This step will simply start the service, call its `predict()` endpoint to get logits, then performs an `argmax` operation to retrieve the class with highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    data: np.ndarray,\n",
    ") -> Output(predictions=list):\n",
    "    \"\"\"Run a inference request against a prediction service\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    prediction = service.predict(data)\n",
    "    prediction = prediction.argmax(axis=-1)\n",
    "    print(f\"Prediction is: {[prediction.tolist()]}\")\n",
    "    return [prediction.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together to initialize and run our inference pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an inference pipeline run\n",
    "my_inference_pipeline = inference_pipeline(\n",
    "    inference_data_loader=inference_data_loader(),\n",
    "    prediction_service_loader=prediction_service_loader(),\n",
    "    predictor=predictor(),\n",
    ")\n",
    "\n",
    "my_inference_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that completes our second ZenBytes chapter on deployment and inference. Our training and deployment pipelines are of course still fairly basic, but we will add in more and more features over the coming lessons.\n",
    "\n",
    "In the next chapter on data management, we will add additional steps for data validation and drift detection to our pipelines, which are important steps to ensure that our model receives the kind of data we expect and does not exhibit training-serving skew."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f70ec6e6bd16014ded89c8222361cbe53cd9507d51ebdcdf3ab6e494d45cf74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
