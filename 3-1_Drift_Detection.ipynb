{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.1: Train-Test Skew Detection with Evidently\n",
    "\n",
    "***Key Concepts:*** *Data-Centric ML, Data Skew, Train-Test Skew, Training-Serving Skew, Evidently*\n",
    "\n",
    "In academia and research, the focus of ML is usually to build the best possible models for a given dataset. However, in practical application, the overall performance of our application is usually determined by the quality of the data, and the model is only secondary. That is why many ML practitioners advocate for **Data-Centric** ML approaches, where we focus on improving the data, while keeping the ML model (mostly) fixed. See also [this great article](https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning) by neptune.ai for more details on model- vs. data-centric approaches.\n",
    "\n",
    "One of the most important parts of data-centric ML is to monitor data quality. Throughout this chapter, we will learn about many potential data issues, such as train-test skew, training-serving skew, data drift, and more. Being aware of these issues, and having respective safety mechanisms in place, is essential when serving ML models to real users.\n",
    "\n",
    "This lesson we will start by automatically checking for **Data Skew** within our ML pipelines. Since the performance of ML models on unseen data can be unpredictable, we should always try to design our training data to match the real environment where our model will later be deployed. The difference between those data distributions is called **Training-Serving Skew**. Similarly, differences in distribution between our training and testing datasets are called **Train-Test Skew**.\n",
    "\n",
    "In the following, we will use the open-source data monitoring tool [Evidently](https://evidentlyai.com/) to measure distribution differences between our datasets. See this little [blog post](https://blog.zenml.io/zenml-loves-evidently/) of ours that explains the evidently integration in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done so, install Evidently by running the following cell, then restart your notebook kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install evidently -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Train-Test Skew\n",
    "\n",
    "To start out, we will use Evidently to check for skew between our training and validation datasets. To do so, we will define a new pipeline with an Evidently step, into which we will then pass our training and validation datasets as . \n",
    "\n",
    "At its core, Evidently’s distribution difference calculation functions take in a reference dataset and compare it with a separate comparison dataset. These are both passed in as [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), though CSV inputs are also possible. ZenML implements this functionality in the form of several standardized steps along with an easy way to use the visualization tools also provided along with Evidently as ‘Dashboards’.\n",
    "\n",
    "Since our datasets were initially in [numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) format before, we just need to add another simple step that converts from numpy to pandas. The overall pipeline will then look like this:\n",
    "\n",
    "![Pipeline2](_assets/3-1/second_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this pipeline in code and import the other steps (which we have already built during previous lessons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.importer import importer\n",
    "from steps.evaluator import evaluator\n",
    "from steps.mlflow_trainer import svc_trainer_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def digits_pipeline_with_train_test_checks(\n",
    "    importer,\n",
    "    trainer,\n",
    "    evaluator,\n",
    "    get_reference_data,\n",
    "    skew_detector,\n",
    "):\n",
    "    \"\"\"Digits pipeline with train-test check.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = importer()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    evaluator(X_test=X_test, y_test=y_test, model=model)\n",
    "    reference, comparison = get_reference_data(X_train, X_test)\n",
    "    skew_detector(reference, comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the two new steps. For data distribution comparison, we can simply use the predefined step of ZenMLs Evidently integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileConfig,\n",
    "    EvidentlyProfileStep,\n",
    ")\n",
    "\n",
    "# configure the Evidently step\n",
    "evidently_profile_config = EvidentlyProfileConfig(\n",
    "    column_mapping=None, profile_sections=[\"datadrift\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step for converting numpy to pandas is also fairly easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "\n",
    "@step\n",
    "def get_reference_data(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    ") -> Output(reference=pd.DataFrame, comparison=pd.DataFrame):\n",
    "    \"\"\"Convert numpy data to pandas for distribution difference calculation.\"\"\"\n",
    "    columns = [str(x) for x in list(range(X_train.shape[1]))]\n",
    "    X_train = pd.DataFrame(X_test, columns=columns)\n",
    "    X_test = pd.DataFrame(X_train, columns=columns)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Let's initialize and run our pipeline to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_pipeline = digits_pipeline_with_train_test_checks(\n",
    "    importer=importer(),\n",
    "    trainer=svc_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    get_reference_data=get_reference_data(),\n",
    "    skew_detector=EvidentlyProfileStep(config=evidently_profile_config),\n",
    ")\n",
    "evidently_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use ZenMLs `EvidentlyVisualizer` to see the distribution comparison right in our notebook, where we can compare the distributions for each feature visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer\n",
    "from zenml.repository import Repository\n",
    "\n",
    "repo = Repository()\n",
    "p = repo.get_pipeline(\"digits_pipeline_with_train_test_checks\")\n",
    "last_run = p.runs[-1]\n",
    "\n",
    "skew_detection_step = last_run.get_step(name=\"skew_detector\")\n",
    "evidently_outputs = skew_detection_step\n",
    "\n",
    "EvidentlyVisualizer().visualize(evidently_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is no skew between our training and validation sets. That's great!\n",
    "\n",
    "In the next lessons, we will add mechanism for training-serving skew and data drift detection into our inference pipelines and will set up automated alerts whenever any data issues were detected. Those lessons are still work in progress, so stay tuned!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f70ec6e6bd16014ded89c8222361cbe53cd9507d51ebdcdf3ab6e494d45cf74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
