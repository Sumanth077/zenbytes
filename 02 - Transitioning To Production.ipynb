{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZenML: Open-source MLOps Framework for reproducible ML pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](_assets/Logo/zenml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> This lesson is still in progress and some commands may not work as described. Please expect an update until 20th April 2022. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging as absl_logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "absl_logging.set_verbosity(-10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by initializing ZenML in our directory. We are going to use a local stack to begin with, for simplicity and then transition to other stacks. This can be achieved in code by executing the following block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize ZenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf .zen\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML handles integrations natively, to avoid dependency conflicts, so make sure to use the following command to install the integrations required for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![All](_assets/integrations_all.png \"All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install kubeflow seldon aws -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Concept of MLOps Stacks\n",
    "\n",
    "The ZenML stack is a concept that describes the union of Metadata Store, Artifact Store and Orchestrator that will be used for all pipeline runs. When you get started with zenml you start off with a default local stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Local Stack\n",
    "\n",
    "You can imagine the local stack to look like this. Within the diagram we show how a generic pipeline interacts with the local stack.\n",
    "\n",
    "![LocalStack](_assets/localstack.png \"LocalStack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kubeflow Pipelines stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the Kubeflow integration to extend the concept of stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to transition to a kubeflow stack that will look a little bit like this. Note that for kubeflow pipelines we also need a registry where the docker images for each step are registered. \n",
    "\n",
    "![KubeflowStack](_assets/aws_stack_redesigned.png \"KubeflowStack\")\n",
    "\n",
    "But we have good news! You barely have to do anything to transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitioning to Production with Kubeflow on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps to follow in order to continue.\n",
    "\n",
    "- Set up the neccessary cloud resources on the provider of your choice\n",
    "- Configure ZenML with a new stack to be able to communicate with these resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up using the cloud guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to continue, it is best to follow the updated cloud guide for ZenML found [here](https://docs.zenml.io/features/guide-aws-gcp-azure). Please return after finishing the `pre-requisites` section.\n",
    "\n",
    "It is recommended you use AWS as your cloud provider to follow along the lesson. However, if you were to select GCP or Azure, it should not so hard to actually modify the below commands to work accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your AWS Kubeflow Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can configure a new stack that points to your newly created resources on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember from the main README, Kubernetes and Docker are a pre-requisite to this part of the guide. Please make sure you have them installed. You also need to install the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) to move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the following with your own configuration. Use the below as exemplary.\n",
    "\n",
    "AWS_EKS_CLUSTER=\"zenhacks-cluster\"\n",
    "AWS_REGION=\"us-east-1\"\n",
    "ECR_REGISTRY_NAME=\"715803424590.dkr.ecr.us-east-1.amazonaws.com\"\n",
    "S3_BUCKET_NAME=\"s3://zenbytes-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register container registry\n",
    "# Point Docker to the registry\n",
    "!aws ecr get-login-password --region {AWS_REGION} | docker login --username AWS --password-stdin {ECR_REGISTRY_NAME}\n",
    "!zenml container-registry register ecr_registry --type=default --uri={ECR_REGISTRY_NAME}\n",
    "\n",
    "# Register orchestrator (Kubeflow on AWS)\n",
    "# if you don't have your kubectl pointing to the eks cluster, run this\n",
    "!aws eks --region {AWS_REGION} update-kubeconfig --name {AWS_EKS_CLUSTER}\n",
    "!zenml orchestrator register eks_orchestrator --type=kubeflow \n",
    "\n",
    "# Register metadata store and artifact store\n",
    "!zenml metadata-store register kubeflow_metadata_store --type=kubeflow\n",
    "!zenml artifact-store register s3_store --type=s3 --path={S3_BUCKET_NAME}\n",
    "\n",
    "\n",
    "# Register a secret manager\n",
    "!zenml secrets-manager register local_secret_manager --type=local\n",
    "\n",
    "# Register the aws_kubeflow_stack\n",
    "!zenml stack register aws_kubeflow_stack -m kubeflow_metadata_store -a s3_store -o eks_orchestrator -c ecr_registry -x local_secret_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition to Production (Run on the Cloud)\n",
    "\n",
    "Once the stack is configured, all that is left to do is to set it active and to run a pipeline. Note that the code itself DOES NOT need to change, only the active stack.\n",
    "\n",
    "ZenML will detect that the stack has changed, and instead of running your pipeline locally, will build a Docker Image, push it to the container registry with your requirements, and deploy the pipeline with that image on Kubeflow Pipelines. This whole process is usually very painful but simplified with ZenML, and is completely customizable.\n",
    "\n",
    "For now, try it out! It might take a few minutes to build and push the image, but after that you'd see your pipeline in the cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Currently running pipelines defined within a jupyter notebook cell is\n",
    "    not supported. To get around this you can run the train pipeline within this repo. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zenml stack set aws_kubeflow_stack\n",
    "\n",
    "# Let's train within kubeflow pipelines - this will deploy the pipeline in a one of manner\n",
    "!python run.py --deploy --kubernetes-context=arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster --namespace=kubeflow --base-url=http://abb84c444c7804aa98fc8c097896479d-377673393.us-east-1.elb.amazonaws.com --interval-second=300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the pipeline run, you should port-forward Kubeflow Pipelines to: [http://localhost:8080/](http://localhost:8080/). You might want to try this is a seperate shell:\n",
    "\n",
    "```\n",
    "kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this only if the port forward from `zenml stack up` did not work. \n",
    "!kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it you have successfully transitioned from local to production by simply switching you ZenML stack. This is just scratching the surface!\n",
    "\n",
    "Next up, more about stacks, running pipelines on a schedule, and much more coming soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
