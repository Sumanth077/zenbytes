{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Artifact Lineage\n",
    "***Key Concepts:*** *Artifacts, Artifact Stores, Metadata Stores, Versioning, Caching*\n",
    "\n",
    "In this lesson we will learn about one of the coolest features of ML pipelines: automatic artifact versioning and tracking. This will give us tremendous insights into how exactly each of our models was created. Furthermore, it enables automatic artifact caching, such that we can switch out parts of our ML pipelines without needing to rerun any of the prior steps.\n",
    "\n",
    "Before we dive into any of this, let's get clear on what exactly **[Artifacts](https://docs.zenml.io/core-concepts#artifact)** are. To illustrate, let us first rebuild our digits pipeline from the previous chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "from src.steps.importer import importer\n",
    "from src.steps.sklearn_trainer import svc_trainer\n",
    "from src.steps.evaluator import evaluator\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def digits_pipeline(importer, trainer, evaluator):\n",
    "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
    "    X_train, X_test, y_train, y_test = importer()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    evaluator(X_test=X_test, y_test=y_test, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artifacts of this pipeline are simply the local variables we defined: `X_train`, `X_test`, `y_train`, `y_test`, and `model`. These make up the data that flows in and out of our steps. In fact, this data is at the core of our pipelines, and the pipeline definition above just defines which artifact is the input or output of what step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Visualization with Dash\n",
    "\n",
    "To visualize how the steps connect the different artifacts, we can view our pipeline with ZenML's [Dash](https://dash.plotly.com/introduction) integration. Run the following code, then open `http://127.0.0.1:8050/` in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install dash -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_svc_pipeline = digits_pipeline(\n",
    "    importer=importer(), trainer=svc_trainer(), evaluator=evaluator()\n",
    ")\n",
    "digits_svc_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer import (\n",
    "    PipelineRunLineageVisualizer,\n",
    ")\n",
    "from zenml.repository import Repository\n",
    "\n",
    "repo = Repository()\n",
    "latest_run = repo.get_pipeline(\"digits_pipeline\").runs[-1]\n",
    "PipelineRunLineageVisualizer().visualize(latest_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see an interactive visualization in your browser as shown below. Squares represent your artifacts, circles your pipeline steps. Also note that the different nodes are color coded, so if your pipeline ever fails or runs for too long, you can find the responsible step at a glance!\n",
    "\n",
    "![Dash Visualization](_assets/02_Artifact_Lineage/dash_initial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact Caching\n",
    "As mentioned in the beginning, tracking which exact artifact went into what steps also allows us to cache and reuse artifacts. Let's see this in action:\n",
    "First, stop the execution of the previous notebook cell in case it is still running. Then, execute the next cell to rerun our pipeline and visualize it with dash again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_svc_pipeline.run()\n",
    "latest_run = repo.get_pipeline(\"digits_pipeline\").runs[-1]\n",
    "PipelineRunLineageVisualizer().visualize(latest_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a visualization as shown below. Note that the color of all nodes in the graph changed to green now. This means they were still cached from our previous run.\n",
    "\n",
    "![Dash Visualization Cached](_assets/02_Artifact_Lineage/dash_cached.png)\n",
    "\n",
    "Let's now replace the SVC model in our ML pipeline with a decision tree and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from zenml.steps import step\n",
    "\n",
    "\n",
    "@step()\n",
    "def tree_trainer(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn decision tree classifier.\"\"\"\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "# redefine and rerun our pipeline, this time with tree_trainer()\n",
    "digits_tree_pipeline = digits_pipeline(\n",
    "    importer=importer(), trainer=tree_trainer(), evaluator=evaluator()\n",
    ")\n",
    "digits_tree_pipeline.run()\n",
    "\n",
    "latest_run = repo.get_pipeline(\"digits_pipeline\").runs[-1]\n",
    "PipelineRunLineageVisualizer().visualize(latest_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization should now look as shown below. Since we changed the trainer, the corresponding node and all subsequent nodes are now blue again, meaning they were rerun and the artifacts were freshly created. However, note how the input data artifacts are still green. They did not have to be recreated. In a real production setting this might save us a tremendous amount of time and resources as those data artifacts might have been the result of some complex, expensive preprocessing job.\n",
    "\n",
    "![Dash Visualization Partly Cached](_assets/02_Artifact_Lineage/dash_partly_cached.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f70ec6e6bd16014ded89c8222361cbe53cd9507d51ebdcdf3ab6e494d45cf74"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('zenbytes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
